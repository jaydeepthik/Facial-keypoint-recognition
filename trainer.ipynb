{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models, datasets\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, glob, cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'P1_Facial_Keypoints' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/udacity/P1_Facial_Keypoints.git\n",
    "!cd P1_Facial_Keypoints\n",
    "root_dir = 'P1_Facial_Keypoints/data/training/'\n",
    "all_img_paths = glob.glob(os.path.join(root_dir, '*.jpg'))\n",
    "data = pd.read_csv('P1_Facial_Keypoints/data/training_frames_keypoints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3462, 137)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape #even cols: x-coordinate, odd cols: y-coordinate // total 68 key points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset class\n",
    "class FacesDataset(Dataset):\n",
    "  def __init__(self, df) -> None:\n",
    "    super().__init__()\n",
    "    self.df = df\n",
    "    self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                              std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "  def __len__(self): return len(self.df)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    image_path = \"P1_Facial_Keypoints/data/training/\" + self.df.iloc[index, 0]\n",
    "    img = cv2.imread(image_path, cv2.COLOR_BGR2RGB)/255.\n",
    "    raw_keypoints = deepcopy(self.df.iloc[index, 1:].tolist())\n",
    "    x_coords = (np.array(raw_keypoints[0::2])/img.shape[1]).tolist()\n",
    "    y_coords = (np.array(raw_keypoints[1::2])/img.shape[0]).tolist()\n",
    "    merged_coords = x_coords + y_coords\n",
    "    merged_coords = torch.tensor(merged_coords)\n",
    "    img = self.perprocess_input(img)\n",
    "    return img, merged_coords\n",
    "\n",
    "  def perprocess_input(self, img):\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = torch.tensor(img).permute(2, 0, 1)\n",
    "    img = self.normalize(img).float()\n",
    "    return img.to(device)\n",
    "\n",
    "  def load_img(self, ix):\n",
    "        img_path = 'P1_Facial_Keypoints/data/training/' + self.df.iloc[ix,0]        \n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)/255.\n",
    "        img = cv2.resize(img, (224,224))\n",
    "        return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=101, shuffle=True)\n",
    "\n",
    "train_dataset = FacesDataset(train_data.reset_index(drop=True))\n",
    "test_dataset = FacesDataset(test_data.reset_index(drop=True))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 32)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "def get_model():\n",
    "  model = models.vgg16(weights='VGG16_Weights.IMAGENET1K_V1')\n",
    "  for param in model.parameters():\n",
    "    param.requires_grads = False\n",
    "  \n",
    "  model.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 136),\n",
    "            nn.Sigmoid()\n",
    "  )\n",
    "  loss_fn = nn.L1Loss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "  return model.to(device), loss_fn, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(imgs, kps, model, loss_fn, optimizer):\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  preds = model(imgs)\n",
    "  loss = loss_fn(preds, kps.to(device))\n",
    "  loss.backward()\n",
    "\n",
    "  optimizer.step()\n",
    "  return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_batch(imgs, kps, model, loss_fn):\n",
    "  model.eval()\n",
    "  preds = model(imgs)\n",
    "  loss = loss_fn(preds, kps.to(device))\n",
    "  return preds, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_model, loss_fn, optimizer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] :  Train Loss: 0.041   Test Loss: 0.033 Time: 282.570538520813\n",
      "Epoch [2] :  Train Loss: 0.032   Test Loss: 0.023 Time: 301.5078809261322\n",
      "Epoch [3] :  Train Loss: 0.026   Test Loss: 0.018 Time: 306.96053767204285\n",
      "Epoch [4] :  Train Loss: 0.026   Test Loss: 0.016 Time: 287.42065262794495\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\AIML\\code\\Facial_kp_detection\\trainer.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/AIML/code/Facial_kp_detection/trainer.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/AIML/code/Facial_kp_detection/trainer.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   imgs, kps \u001b[39m=\u001b[39m batch\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/AIML/code/Facial_kp_detection/trainer.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   batch_loss \u001b[39m=\u001b[39m train_batch(imgs, kps, VGG_model, loss_fn, optimizer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/AIML/code/Facial_kp_detection/trainer.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m   epoch_train_loss\u001b[39m.\u001b[39mappend(batch_loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/AIML/code/Facial_kp_detection/trainer.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m train_loss\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(epoch_train_loss))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "train_loss, test_loss = [], []\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  start = time.time()\n",
    "  epoch_train_loss, epoch_test_loss = [],  []\n",
    "\n",
    "  for idx, batch in enumerate(train_dataloader):\n",
    "    imgs, kps = batch\n",
    "    batch_loss = train_batch(imgs, kps, VGG_model, loss_fn, optimizer)\n",
    "    epoch_train_loss.append(batch_loss)\n",
    "  train_loss.append(np.mean(epoch_train_loss))\n",
    "\n",
    "  for idx, batch in enumerate(test_dataloader):\n",
    "    imgs, kps = batch\n",
    "    batch_preds, batch_loss = validate_batch(imgs, kps, VGG_model, loss_fn)\n",
    "    epoch_test_loss.append(batch_loss)\n",
    "  test_loss.append(np.mean(epoch_test_loss))\n",
    "\n",
    "  print(f\"Epoch [{epoch+1}] :  Train Loss: {epoch_train_loss[-1]:0.3f}   Test Loss: {epoch_test_loss[-1]:0.3f} Time: {time.time() - start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
